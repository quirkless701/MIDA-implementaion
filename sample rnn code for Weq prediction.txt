import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

# Define the RNN model
class WalrasianRNN(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(WalrasianRNN, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        # Initialize hidden and cell states
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        
        # Forward pass through the RNN
        out, _ = self.rnn(x, (h0, c0))
        
        # Use the last hidden state for the output
        out = self.fc(out[:, -1, :])
        return out

# Hyperparameters
input_size = 1  # Each feature is a single value in the sequence
hidden_size = 64
num_layers = 2
output_size = 1  # Predicting one equilibrium price
learning_rate = 0.001
num_epochs = 100

# Instantiate the model
model = WalrasianRNN(input_size, hidden_size, num_layers, output_size)
model = model.to(torch.device("cuda" if torch.cuda.is_available() else "cpu"))

# Define loss function and optimizer
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Convert the data into PyTorch tensors
def prepare_data(data):
    sequences = []
    labels = []

    for market_details, equilibrium_price in data:
        # Convert market details into a sequence of features
        sequence = torch.tensor(market_details, dtype=torch.float32).unsqueeze(-1)  # Shape: (sequence_length, 1)
        label = torch.tensor([equilibrium_price], dtype=torch.float32)  # Shape: (1,)
        sequences.append(sequence)
        labels.append(label)

    return sequences, labels

# Generate the data
num_samples = 1000
synthetic_data = generate_limited_supply_data(num_samples)
sequences, labels = prepare_data(synthetic_data)

# Split the data into training and validation sets
train_size = int(0.8 * len(sequences))
train_sequences = sequences[:train_size]
train_labels = labels[:train_size]
val_sequences = sequences[train_size:]
val_labels = labels[train_size:]

# Create DataLoader objects for batching
train_data = list(zip(train_sequences, train_labels))
val_data = list(zip(val_sequences, val_labels))
train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)
val_loader = torch.utils.data.DataLoader(val_data, batch_size=32, shuffle=False)

# Training loop
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0

    for sequences, labels in train_loader:
        sequences = torch.stack(sequences).to(device)  # Shape: (batch_size, sequence_length, input_size)
        labels = torch.stack(labels).to(device)  # Shape: (batch_size, 1)

        # Forward pass
        outputs = model(sequences)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    # Validation loss
    model.eval()
    val_loss = 0.0
    with torch.no_grad():
        for sequences, labels in val_loader:
            sequences = torch.stack(sequences).to(device)
            labels = torch.stack(labels).to(device)

            outputs = model(sequences)
            loss = criterion(outputs, labels)
            val_loss += loss.item()

    print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {running_loss / len(train_loader):.4f}, '
          f'Validation Loss: {val_loss / len(val_loader):.4f}')

print("Training complete!")
